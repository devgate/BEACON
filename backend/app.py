from flask import Flask, render_template, request, jsonify, send_file
from flask_cors import CORS
from datetime import datetime
import random
import os
from werkzeug.utils import secure_filename
import PyPDF2
import io
from pdf2image import convert_from_bytes
import logging
import time
import json
from typing import Optional

# Configure logging first
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import our RAG components
from services.bedrock_service import BedrockService, create_bedrock_service
from storage.vector_store import VectorStore, create_vector_store
from services.rag_engine import RAGEngine, create_rag_engine

# Import new document processing components (with fallback)
try:
    from core.document_processor import DocumentProcessor
    from storage.chroma_service import ChromaService, DocumentChunker
    ENHANCED_PROCESSING_AVAILABLE = True
    logger.info("Enhanced document processing modules loaded successfully")
except ImportError as e:
    logger.warning(f"Enhanced document processing not available: {e}")
    DocumentProcessor = None
    ChromaService = None
    DocumentChunker = None
    ENHANCED_PROCESSING_AVAILABLE = False

app = Flask(__name__)
CORS(app, origins=['http://localhost:3000', 'http://localhost:8080'])
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB ìµœëŒ€ íŒŒì¼ í¬ê¸°
app.config['UPLOAD_FOLDER'] = 'uploads'

# Initialize RAG system and new components
try:
    logger.info("Initializing enhanced document processing system...")
    
    # Initialize Bedrock service
    bedrock_service = create_bedrock_service({
        'BEDROCK_REGION': os.getenv('BEDROCK_REGION', 'ap-northeast-2'),
        'AWS_PROFILE': os.getenv('AWS_PROFILE')
    })
    
    # Initialize vector store (legacy)
    vector_store = create_vector_store(
        table_name=os.getenv('DYNAMODB_VECTORS_TABLE', 'prod-beacon-vectors')
    )
    
    # Initialize RAG engine (legacy)
    rag_engine = create_rag_engine(bedrock_service, vector_store)
    
    # Initialize new components (if available)
    document_processor = None
    chroma_service = None
    
    if ENHANCED_PROCESSING_AVAILABLE:
        try:
            document_processor = DocumentProcessor()
            chroma_service = ChromaService(persist_directory=os.getenv('CHROMA_DATA_DIR', 'chroma_data'))
            logger.info("Enhanced document processing system initialized successfully")
            CHROMA_ENABLED = True
        except Exception as e:
            logger.warning(f"Failed to initialize enhanced processing: {e}")
            CHROMA_ENABLED = False
    else:
        logger.info("Enhanced processing not available, using legacy mode")
        CHROMA_ENABLED = False
    
    RAG_ENABLED = True
    
except Exception as e:
    logger.error(f"Failed to initialize document processing system: {e}")
    logger.info("Running in mock mode")
    RAG_ENABLED = False
    CHROMA_ENABLED = False
    bedrock_service = None
    vector_store = None
    rag_engine = None
    document_processor = None
    chroma_service = None

# Mock AI ì‘ë‹µ ë°ì´í„°
MOCK_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! BEACON AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    "ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•œ ê²°ê³¼, ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
    "ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê²€í† í•´ë³´ë‹ˆ í¥ë¯¸ë¡œìš´ ë‚´ìš©ì´ ë§ë„¤ìš”. íŠ¹ì • ì„¹ì…˜ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "ë¬¸ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½í•˜ìë©´, í•µì‹¬ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
    "í•´ë‹¹ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ë„¤, ì°¾ì•˜ìŠµë‹ˆë‹¤! ìƒì„¸í•œ ì„¤ëª…ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
]

# ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì‘ë‹µ
CATEGORY_RESPONSES = {
    1: ["ì¬ë¬´ ë¬¸ì„œë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.", "ì¬ì • ìƒíƒœê°€ ì–‘í˜¸í•´ ë³´ì…ë‹ˆë‹¤.", "íšŒê³„ ê¸°ì¤€ì— ë”°ë¥´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."],
    2: ["ë§›ì§‘ ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.", "ì´ ìŒì‹ì ì˜ íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.", "ì¶”ì²œ ë©”ë‰´ì™€ ê°€ê²© ì •ë³´ì…ë‹ˆë‹¤."],
    3: ["ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•œ ê²°ê³¼ì…ë‹ˆë‹¤.", "ì‚¬ìš©ë²•ì€ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”.", "ì£¼ì˜ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”."],
    4: ["ë¬¸ì„œ ë‚´ìš©ì„ ê²€í† í–ˆìŠµë‹ˆë‹¤.", "ê´€ë ¨ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.", "ì¶”ê°€ ì°¸ê³ ì‚¬í•­ì…ë‹ˆë‹¤."]
}

# í—ˆìš©ëœ íŒŒì¼ í™•ì¥ì (í™•ì¥ë¨)
ALLOWED_EXTENSIONS = {'pdf', 'txt', 'docx', 'doc', 'xlsx', 'xls', 'csv', 'json', 'md', 'rtf'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_pdf(file_stream):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(file_stream)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def generate_embeddings(texts, batch_size=10):
    """
    Generate embeddings for a list of texts using Bedrock
    
    Args:
        texts: List of text strings
        batch_size: Number of texts to process in each batch
        
    Returns:
        List of embedding vectors
    """
    if not RAG_ENABLED or not bedrock_service:
        logger.warning("Bedrock service not available, returning mock embeddings")
        return [[0.0] * 1536 for _ in texts]  # Mock embeddings
    
    try:
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = []
            
            for text in batch:
                # Truncate text if too long
                if len(text) > 8000:
                    text = text[:8000]
                
                embedding = bedrock_service.generate_embedding(text)
                batch_embeddings.append(embedding)
            
            embeddings.extend(batch_embeddings)
            
            # Add small delay to avoid rate limiting
            if i + batch_size < len(texts):
                time.sleep(0.1)
        
        logger.info(f"Generated {len(embeddings)} embeddings")
        return embeddings
        
    except Exception as e:
        logger.error(f"Failed to generate embeddings: {e}")
        # Return mock embeddings as fallback
        return [[0.0] * 1536 for _ in texts]

def extract_images_from_pdf(file_stream, document_id):
    """
    PDF íŒŒì¼ì—ì„œ ê° í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_stream: PDF íŒŒì¼ì˜ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼
        document_id: ë¬¸ì„œ ID (ë””ë ‰í† ë¦¬ êµ¬ë¶„ìš©)
        
    Returns:
        list: ì¶”ì¶œëœ ì´ë¯¸ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (í˜ì´ì§€ ë²ˆí˜¸, íŒŒì¼ëª…, URL, ê²½ë¡œ í¬í•¨)
    """
    images = []
    try:
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (pdf2image ì‚¬ìš©)
        file_stream.seek(0)  # ìŠ¤íŠ¸ë¦¼ ìœ„ì¹˜ ì´ˆê¸°í™”
        pdf_images = convert_from_bytes(file_stream.read(), dpi=150, fmt='PNG')
        
        # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë¬¸ì„œë³„ë¡œ ë¶„ë¦¬)
        image_dir = os.path.join('static', 'images', f'doc_{document_id}')
        os.makedirs(image_dir, exist_ok=True)
        
        # ê° í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
        for page_num, image in enumerate(pdf_images, 1):
            # ì´ë¯¸ì§€ íŒŒì¼ëª… ìƒì„± (í˜ì´ì§€_ë²ˆí˜¸.png)
            image_filename = f'page_{page_num}.png'
            image_path = os.path.join(image_dir, image_filename)
            
            # ì´ë¯¸ì§€ë¥¼ PNG í˜•ì‹ìœ¼ë¡œ ì €ì¥
            image.save(image_path, 'PNG')
            
            # ì›¹ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ URL ìƒì„±
            image_url = f'/static/images/doc_{document_id}/{image_filename}'
            
            # ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            images.append({
                'page': page_num,
                'filename': image_filename,
                'url': image_url,
                'path': image_path
            })
            
        print(f"PDF {document_id}: {len(images)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return images
        
    except Exception as e:
        print(f"PDF ì´ë¯¸ì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

# ë¬¸ì„œ ë°ì´í„° ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
# ì—…ë¡œë“œëœ PDF ë¬¸ì„œë“¤ì˜ ë©”íƒ€ë°ì´í„°ì™€ ë‚´ìš©ì„ ì €ì¥
documents = []

# ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ì €ì¥ì†Œ
# ê° ì˜ì—­ë³„ë¡œ ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ì—¬ ê´€ë¦¬
categories = [
    {
        "id": 1, 
        "name": "ì¬ë¬´", 
        "description": "ì¬ë¬´ ê´€ë ¨ ë¬¸ì„œ", 
        "icon": "fas fa-calculator", 
        "color": "#10B981",
        "settings": {
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
            "chunk_size": 512,
            "chunk_overlap": 50,
            "chunk_strategy": "sentence"
        }
    },
    {
        "id": 2, 
        "name": "ë§›ì§‘", 
        "description": "ë§›ì§‘ ì •ë³´ ë¬¸ì„œ", 
        "icon": "fas fa-utensils", 
        "color": "#F59E0B",
        "settings": {
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
            "chunk_size": 256,
            "chunk_overlap": 30,
            "chunk_strategy": "paragraph"
        }
    },
    {
        "id": 3, 
        "name": "ë§¤ë‰´ì–¼", 
        "description": "ì‚¬ìš© ì„¤ëª…ì„œ ë° ë§¤ë‰´ì–¼", 
        "icon": "fas fa-book", 
        "color": "#3B82F6",
        "settings": {
            "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "chunk_size": 1024,
            "chunk_overlap": 100,
            "chunk_strategy": "section"
        }
    },
    {
        "id": 4, 
        "name": "ì¼ë°˜", 
        "description": "ê¸°íƒ€ ë¬¸ì„œ", 
        "icon": "fas fa-file-alt", 
        "color": "#6B7280",
        "settings": {
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
            "chunk_size": 512,
            "chunk_overlap": 50,
            "chunk_strategy": "sentence"
        }
    }
]

# ë¬¸ì„œ ID ìë™ ì¦ê°€ ì¹´ìš´í„°
# ìƒˆë¡œìš´ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ ê³ ìœ  ID ìƒì„±ì— ì‚¬ìš©
document_counter = 0

# ì¹´í…Œê³ ë¦¬ ID ìë™ ì¦ê°€ ì¹´ìš´í„°
category_counter = len(categories)

# ì±„íŒ… ê¸°ë¡ ì €ì¥ì†Œ
# ì‚¬ìš©ìì™€ AI ê°„ì˜ ëŒ€í™” ë‚´ì—­ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì €ì¥
chat_history = []

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/documents')
def get_documents():
    """ì „ì²´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    return jsonify(documents)

@app.route('/api/categories')
def get_categories():
    """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ê°œìˆ˜ ì¶”ê°€
    categories_with_count = []
    for category in categories:
        doc_count = len([doc for doc in documents if doc.get('category_id') == category['id']])
        category_with_count = category.copy()
        category_with_count['document_count'] = doc_count
        categories_with_count.append(category_with_count)
    
    return jsonify(categories_with_count)

@app.route('/api/categories/<int:category_id>/documents')
def get_documents_by_category(category_id):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    category_documents = [doc for doc in documents if doc.get('category_id') == category_id]
    return jsonify(category_documents)

@app.route('/api/categories', methods=['POST'])
def create_category():
    """ìƒˆ ì¹´í…Œê³ ë¦¬ ìƒì„±"""
    global category_counter
    
    data = request.get_json()
    name = data.get('name', '').strip()
    description = data.get('description', '').strip()
    icon = data.get('icon', 'fas fa-folder')
    color = data.get('color', '#6B7280')
    
    if not name:
        return jsonify({'error': 'ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
    
    # ì¤‘ë³µ ì´ë¦„ ì²´í¬
    if any(cat['name'] == name for cat in categories):
        return jsonify({'error': 'ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì´ë¦„ì…ë‹ˆë‹¤.'}), 400
    
    category_counter += 1
    new_category = {
        'id': category_counter,
        'name': name,
        'description': description,
        'icon': icon,
        'color': color
    }
    
    categories.append(new_category)
    
    return jsonify({
        'success': True,
        'category': new_category,
        'message': f'ì¹´í…Œê³ ë¦¬ "{name}"ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.'
    })

@app.route('/api/categories/<int:category_id>/settings', methods=['PUT'])
def update_category_settings(category_id):
    """ì¹´í…Œê³ ë¦¬ë³„ RAG ì„¤ì • ì—…ë°ì´íŠ¸"""
    data = request.get_json()
    
    # ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
    category = next((cat for cat in categories if cat['id'] == category_id), None)
    if not category:
        return jsonify({'error': 'ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
    
    # ì„¤ì • ì—…ë°ì´íŠ¸
    if 'embedding_model' in data:
        category['settings']['embedding_model'] = data['embedding_model']
    if 'chunk_size' in data:
        category['settings']['chunk_size'] = int(data['chunk_size'])
    if 'chunk_overlap' in data:
        category['settings']['chunk_overlap'] = int(data['chunk_overlap'])
    if 'chunk_strategy' in data:
        category['settings']['chunk_strategy'] = data['chunk_strategy']
    
    return jsonify({
        'success': True,
        'category': category,
        'message': f'ì¹´í…Œê³ ë¦¬ "{category["name"]}" ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.'
    })

@app.route('/api/bedrock/models')
def get_bedrock_models():
    """Get available Bedrock models"""
    if not RAG_ENABLED:
        return jsonify({
            'error': 'RAG system not available',
            'models': []
        }), 503
    
    try:
        models = bedrock_service.get_available_models(include_inference_profiles=True)
        model_list = []
        
        for model in models:
            model_dict = model.to_dict()
            model_list.append(model_dict)
        
        return jsonify({
            'models': model_list,
            'total_count': len(model_list),
            'inference_profiles_included': True
        })
        
    except Exception as e:
        logger.error(f"Error getting Bedrock models: {e}")
        return jsonify({
            'error': 'Failed to retrieve Bedrock models',
            'details': str(e)
        }), 500

@app.route('/api/bedrock/health')
def bedrock_health():
    """Health check for Bedrock and RAG system"""
    if not RAG_ENABLED:
        return jsonify({
            'status': 'unavailable',
            'message': 'RAG system not initialized',
            'rag_enabled': False
        })
    
    try:
        health_status = rag_engine.health_check()
        return jsonify({
            'status': health_status['status'],
            'rag_enabled': True,
            'details': health_status
        })
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return jsonify({
            'status': 'unhealthy',
            'rag_enabled': False,
            'error': str(e)
        }), 500

@app.route('/api/embedding-models')
def get_embedding_models():
    """Get available embedding models (legacy endpoint)"""
    if RAG_ENABLED:
        # Return Bedrock embedding models
        models = [
            {
                "id": "amazon.titan-embed-text-v1",
                "name": "Titan Text Embeddings v1",
                "description": "Amazon's high-quality text embedding model",
                "language": "multilingual",
                "dimension": 1536,
                "provider": "amazon"
            },
            {
                "id": "amazon.titan-embed-text-v2",
                "name": "Titan Text Embeddings v2",
                "description": "Improved Amazon text embedding model",
                "language": "multilingual", 
                "dimension": 1024,
                "provider": "amazon"
            }
        ]
    else:
        # Fallback to original models
        models = [
            {
                "id": "sentence-transformers/all-MiniLM-L6-v2",
                "name": "All-MiniLM-L6-v2",
                "description": "ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë²”ìš© ëª¨ë¸",
                "language": "multilingual",
                "size": "80MB"
            },
            {
                "id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "name": "Paraphrase-Multilingual-MiniLM-L12-v2",
                "description": "ë‹¤êµ­ì–´ ì§€ì› ê³ í’ˆì§ˆ ëª¨ë¸",
                "language": "multilingual",
                "size": "420MB"
            }
        ]
    
    return jsonify(models)

@app.route('/api/chat', methods=['POST'])
def chat():
    """
    Enhanced chat API endpoint with Bedrock RAG integration
    Supports model selection, cost tracking, and real AI responses
    """
    logger.info("ğŸ“ Chat API í˜¸ì¶œë¨ - ìš”ì²­ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    data = request.get_json()
    user_message = data.get('message', '')
    selected_category_id = data.get('category_id', None)
    model_id = data.get('model_id', None)
    settings = data.get('settings', {})
    
    # Extract settings
    temperature = float(settings.get('temperature', 0.7))
    max_tokens = int(settings.get('max_tokens', 2048))
    use_rag = settings.get('use_rag', True)
    knowledge_base_id = settings.get('knowledge_base_id', None)
    top_k_documents = int(settings.get('top_k_documents', 5))
    
    try:
        logger.info("ğŸ” RAG_ENABLED ìƒíƒœ: %s", RAG_ENABLED)
        if RAG_ENABLED:
            # Check if ChromaDB RAG should be used (when knowledge_base_id is provided)
            logger.info("ğŸ” RAG ì¡°ê±´ ì²´í¬: use_rag=%s, knowledge_base_id=%s, CHROMA_ENABLED=%s, chroma_service=%s", 
                       use_rag, knowledge_base_id, CHROMA_ENABLED, chroma_service is not None)
            use_chroma_rag = use_rag and knowledge_base_id and CHROMA_ENABLED and chroma_service
            logger.info("ğŸ¯ ChromaDB RAG ì‚¬ìš© ì—¬ë¶€: %s", use_chroma_rag)
            
            if use_chroma_rag:
                # Use ChromaDB RAG system
                logger.info("=== RAG ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
                logger.info("1. âœ… ì§ˆë¬¸ ì…ë ¥: %s", user_message[:50] + "..." if len(user_message) > 50 else user_message)
                
                import time
                start_time = time.time()
                
                # Generate embedding for the user query
                logger.info("2. ğŸ”„ ì§ˆë¬¸ Bedrock ì„ë² ë”© ìƒì„± ì‹œì‘")
                query_embeddings = generate_embeddings([user_message])
                if not query_embeddings or len(query_embeddings) == 0:
                    raise Exception("Failed to generate query embedding")
                
                query_embedding = query_embeddings[0]
                logger.info("2. âœ… ì§ˆë¬¸ Bedrock ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                
                # Search similar chunks in ChromaDB filtered by knowledge base
                logger.info("3. ğŸ”„ ChromaDB ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘ - ì§€ì‹ë² ì´ìŠ¤: %s", knowledge_base_id)
                search_results = chroma_service.search_similar_chunks(
                    query_embedding=query_embedding,
                    n_results=top_k_documents,
                    document_filter=f"kb_{knowledge_base_id}_doc_"  # Filter by knowledge base
                )
                
                # Check if we found relevant documents
                if search_results['total_results'] > 0:
                    logger.info("3. âœ… ChromaDB ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ - %dê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬", search_results['total_results'])
                    
                    # Build context from retrieved chunks
                    context_chunks = search_results['chunks']
                    metadatas = search_results['metadatas']
                    distances = search_results['distances']
                    
                    logger.info("4. ğŸ”„ ê²€ìƒ‰ëœ ì²­í¬ë¡œ LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹œì‘")
                    # Format context for LLM
                    context = "\n\n".join([
                        f"ë¬¸ì„œ '{meta.get('document_name', 'Unknown')}' - ê´€ë ¨ë„: {1-distance:.3f}\n{chunk}"
                        for chunk, meta, distance in zip(context_chunks, metadatas, distances)
                    ])
                    
                    # Set default model if not specified
                    if not model_id:
                        available_models = bedrock_service.get_available_models()
                        # Filter for text generation models
                        text_models = [
                            model for model in available_models 
                            if 'TEXT' in model.output_modalities and 'EMBEDDING' not in model.output_modalities
                        ]
                        if text_models:
                            # Prefer Claude models, then Amazon Nova, then others
                            claude_models = [m for m in text_models if 'claude' in m.model_id.lower()]
                            nova_models = [m for m in text_models if 'nova' in m.model_id.lower()]
                            
                            if claude_models:
                                model_id = claude_models[0].model_id
                            elif nova_models:
                                model_id = nova_models[0].model_id
                            else:
                                model_id = text_models[0].model_id
                        else:
                            raise Exception("No text generation models available")
                    
                    # Create RAG system prompt
                    system_prompt = f"""ë‹¹ì‹ ì€ BEACON AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”
4. ê°€ëŠ¥í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”
5. ê´€ë ¨ëœ ë¬¸ì„œ ì„¹ì…˜ì´ë‚˜ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” í•´ë‹¹ ë¬¸ì„œëª…ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”"""
                    
                    logger.info("4. âœ… LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ")
                    logger.info("5. ğŸ”„ Bedrock LLM ì‘ë‹µ ìƒì„± ì‹œì‘ - ëª¨ë¸: %s", model_id)
                    
                    # Generate response using Bedrock with context
                    response_data = bedrock_service.invoke_model(
                        model_id=model_id,
                        prompt=user_message,
                        system_prompt=system_prompt,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                    
                    logger.info("5. âœ… Bedrock LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
                    
                    processing_time = time.time() - start_time
                    
                    # Format referenced docs
                    referenced_docs = []
                    for meta, distance in zip(metadatas, distances):
                        referenced_docs.append({
                            'id': meta.get('document_id', 'unknown'),
                            'title': meta.get('document_name', 'Unknown Document'),
                            'has_file': True,
                            'relevance_score': 1 - distance,
                            'chunk_index': meta.get('chunk_index', 0)
                        })
                    
                    # Save to chat history
                    chat_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'user_message': user_message,
                        'ai_response': response_data['text'],
                        'model_used': model_id,
                        'knowledge_base_id': knowledge_base_id,
                        'tokens_used': response_data.get('usage', {}),
                        'cost_estimate': response_data.get('cost', {}),
                        'confidence_score': sum(1-d for d in distances) / len(distances) if distances else 0.0,
                        'processing_time': processing_time,
                        'sources_count': len(context_chunks)
                    }
                    chat_history.append(chat_entry)
                    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ")
                    
                    return jsonify({
                        'response': response_data['text'],
                        'model_used': model_id,
                        'timestamp': chat_entry['timestamp'],
                        'tokens_used': response_data.get('usage', {}),
                        'cost_estimate': response_data.get('cost', {}),
                        'confidence_score': chat_entry['confidence_score'],
                        'processing_time': processing_time,
                        'images': [],
                        'referenced_docs': referenced_docs[:3],
                        'rag_enabled': True
                    })
                else:
                    # No relevant documents found in ChromaDB, fall back to general response
                    logger.warning(f"No relevant documents found in ChromaDB for query: {user_message[:50]}...")
                    
                    # Fall back to general conversation with Bedrock
                    logger.info(f"Falling back to general conversation with Bedrock: {user_message[:50]}...")
                    
                    import time
                    start_time = time.time()
                    
                    # Set default model if not specified
                    if not model_id:
                        available_models = bedrock_service.get_available_models()
                        # Filter for text generation models (exclude embedding models)
                        text_models = [
                            model for model in available_models 
                            if 'TEXT' in model.output_modalities and 'EMBEDDING' not in model.output_modalities
                        ]
                        if text_models:
                            # Prefer Claude models, then Amazon Nova, then others
                            claude_models = [m for m in text_models if 'claude' in m.model_id.lower()]
                            nova_models = [m for m in text_models if 'nova' in m.model_id.lower()]
                            
                            if claude_models:
                                model_id = claude_models[0].model_id
                            elif nova_models:
                                model_id = nova_models[0].model_id
                            else:
                                model_id = text_models[0].model_id
                        else:
                            raise Exception("No text generation models available")
                    
                    # Create a general conversation system prompt
                    system_prompt = """ë‹¹ì‹ ì€ BEACON AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”. 
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ë˜, ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ ì–¸ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§ì”€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìê°€ ë¬¸ì„œë‚˜ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í–ˆì§€ë§Œ, ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. 
ì´ ê²½ìš° ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ê³  ì¼ë°˜ì ì¸ ì •ë³´ë‚˜ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
                    
                    # Invoke Bedrock model directly
                    response_data = bedrock_service.invoke_model(
                        model_id=model_id,
                        prompt=user_message,
                        system_prompt=system_prompt,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                    
                    processing_time = time.time() - start_time
                    
                    # Save to chat history
                    chat_entry = {
                        'timestamp': datetime.now().isoformat(),
                        'user_message': user_message,
                        'ai_response': response_data['text'],
                        'model_used': model_id,
                        'category_id': selected_category_id,
                        'knowledge_base_id': knowledge_base_id,
                        'tokens_used': response_data.get('usage', {}),
                        'cost_estimate': response_data.get('cost', {}),
                        'confidence_score': 0.5,  # Lower confidence since no docs found
                        'processing_time': processing_time,
                        'rag_enabled': True,
                        'sources_count': 0
                    }
                    chat_history.append(chat_entry)
                    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (ë¬¸ì„œ ì—†ìŒ - ì¼ë°˜ ì‘ë‹µ)")
                    
                    return jsonify({
                        'response': response_data['text'],
                        'model_used': model_id,
                        'timestamp': chat_entry['timestamp'],
                        'tokens_used': response_data.get('usage', {}),
                        'cost_estimate': response_data.get('cost', {}),
                        'confidence_score': chat_entry['confidence_score'],
                        'processing_time': processing_time,
                        'images': [],
                        'referenced_docs': [],
                        'rag_enabled': True,
                        'sources_found': False
                    })
            
            elif use_rag and documents:
                # Use legacy RAG system with uploaded documents
                logger.info(f"Processing legacy RAG query: {user_message[:50]}... with model: {model_id}")
                
                response_data = rag_engine.query(
                    query_text=user_message,
                    category_id=selected_category_id,
                    model_id=model_id,
                    top_k_documents=top_k_documents,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    include_sources=True
                )
                
                # Format sources for frontend
                referenced_docs = []
                images = []
                
                for source in response_data.sources:
                    # Find corresponding document for metadata
                    doc_info = next((doc for doc in documents if str(doc['id']) == source.document_id), None)
                    
                    referenced_docs.append({
                        'id': source.document_id,
                        'title': source.metadata.get('title', 'Unknown Document'),
                        'has_file': bool(doc_info and doc_info.get('file_path')),
                        'relevance_score': source.similarity_score,
                        'chunk_index': source.chunk_index
                    })
                    
                    # Add images if available
                    if doc_info and doc_info.get('images'):
                        images.extend(doc_info['images'][:2])  # Limit to 2 images per source
                
                # Save to chat history
                chat_entry = {
                    'timestamp': datetime.now().isoformat(),
                    'user_message': user_message,
                    'ai_response': response_data.response,
                    'model_used': response_data.model_used,
                    'category_id': selected_category_id,
                    'tokens_used': response_data.tokens_used,
                    'cost_estimate': response_data.cost_estimate,
                    'confidence_score': response_data.confidence_score,
                    'processing_time': response_data.processing_time,
                    'sources_count': len(response_data.sources)
                }
                chat_history.append(chat_entry)
                logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (RAG ë¹„í™œì„±í™”)")
                
                return jsonify({
                    'response': response_data.response,
                    'model_used': response_data.model_used,
                    'timestamp': chat_entry['timestamp'],
                    'tokens_used': response_data.tokens_used,
                    'cost_estimate': response_data.cost_estimate,
                    'confidence_score': response_data.confidence_score,
                    'processing_time': response_data.processing_time,
                    'images': images[:5],  # Limit to 5 images total
                    'referenced_docs': referenced_docs[:3],  # Limit to 3 documents
                    'rag_enabled': True
                })
            # Use Bedrock for general conversation without RAG (or ChromaDB had no results)
            if not use_chroma_rag and not (use_rag and documents):
                logger.info(f"Processing general conversation with Bedrock: {user_message[:50]}... with model: {model_id}")
                
                import time
                start_time = time.time()
                
                # Set default model if not specified
                if not model_id:
                    available_models = bedrock_service.get_available_models()
                    # Filter for text generation models (exclude embedding models)
                    text_models = [
                        model for model in available_models 
                        if 'TEXT' in model.output_modalities and 'EMBEDDING' not in model.output_modalities
                    ]
                    if text_models:
                        # Prefer Claude models, then Amazon Nova, then others
                        claude_models = [m for m in text_models if 'claude' in m.model_id.lower()]
                        nova_models = [m for m in text_models if 'nova' in m.model_id.lower()]
                        
                        if claude_models:
                            model_id = claude_models[0].model_id
                        elif nova_models:
                            model_id = nova_models[0].model_id
                        else:
                            model_id = text_models[0].model_id
                    else:
                        raise Exception("No text generation models available")
                
                # Create a general conversation system prompt
                system_prompt = """ë‹¹ì‹ ì€ BEACON AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”. 
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ë˜, ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ ì–¸ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§ì”€í•´ì£¼ì„¸ìš”."""
                
                # Invoke Bedrock model directly
                response_data = bedrock_service.invoke_model(
                    model_id=model_id,
                    prompt=user_message,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                processing_time = time.time() - start_time
                
                # Save to chat history
                chat_entry = {
                    'timestamp': datetime.now().isoformat(),
                    'user_message': user_message,
                    'ai_response': response_data['text'],
                    'model_used': model_id,
                    'category_id': selected_category_id,
                    'tokens_used': response_data.get('usage', {}),
                    'cost_estimate': response_data.get('cost', {}),
                    'confidence_score': 1.0,  # General conversation doesn't have relevance score
                    'processing_time': processing_time,
                    'sources_count': 0
                }
                chat_history.append(chat_entry)
                logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (ì´ë¯¸ì§€ í¬í•¨)")
                
                return jsonify({
                    'response': response_data['text'],
                    'model_used': model_id,
                    'timestamp': chat_entry['timestamp'],
                    'tokens_used': response_data.get('usage', {}),
                    'cost_estimate': response_data.get('cost', {}),
                    'confidence_score': 1.0,
                    'processing_time': processing_time,
                    'images': [],
                    'referenced_docs': [],
                    'rag_enabled': False
                })
        else:
            # RAG_ENABLED is False - fallback to mock responses
            logger.warning("âŒ RAG_ENABLEDê°€ Falseì…ë‹ˆë‹¤ - Mock ì‘ë‹µ ì‹œìŠ¤í…œ ì‚¬ìš©")
            return _generate_mock_response(user_message, selected_category_id)
            
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}")
        # Fallback to mock response on error
        return _generate_mock_response(user_message, selected_category_id, error=str(e))

def _generate_mock_response(user_message: str, category_id: Optional[int] = None, error: str = None):
    """Generate mock response for fallback mode"""
    try:
        # Add error context if provided
        if error:
            logger.warning(f"Generating mock response due to error: {error}")
        
        # ê´€ë ¨ ë¬¸ì„œ ë° ì´ë¯¸ì§€ ì €ì¥ì†Œ ì´ˆê¸°í™”
        relevant_docs = []
        relevant_images = []
        
        # ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ í•„í„°ë§ (ì¹´í…Œê³ ë¦¬ê°€ ì„ íƒëœ ê²½ìš° í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ)
        search_documents = documents
        if category_id:
            search_documents = [doc for doc in documents if doc.get('category_id') == category_id]
            logger.info(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬ {category_id}ì˜ ë¬¸ì„œ {len(search_documents)}ê°œë¥¼ ê²€ìƒ‰ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •")
        
        # ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
        if len(search_documents) == 0:
            if category_id:
                # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ ì´ë¦„ ì°¾ê¸°
                category_name = next((cat['name'] for cat in categories if cat['id'] == category_id), 'ì„ íƒëœ ì¹´í…Œê³ ë¦¬')
                response = f"í˜„ì¬ '{category_name}' ì¹´í…Œê³ ë¦¬ì— ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•œ í›„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            else:
                response = "í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•œ í›„ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        else:
            # ì¸ì‚¬ë§ ë° ê°„ë‹¨í•œ ì§ˆë¬¸ í•„í„°ë§
            # ì´ëŸ° ê²½ìš°ì—ëŠ” ë¬¸ì„œ ê²€ìƒ‰ì„ í•˜ì§€ ì•Šê³  ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±
            greeting_words = ['ì•ˆë…•', 'hello', 'hi', 'ë°˜ê°€ì›Œ', 'ì˜ì§€ë‚´', 'ì–´ë–»ê²Œ']
            simple_questions = ['ë­ì•¼', 'ë­”ê°€', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ']
            
            user_message_lower = user_message.lower()
            is_greeting = any(word in user_message_lower for word in greeting_words)
            is_simple = len(user_message.split()) <= 3 and any(word in user_message_lower for word in simple_questions)
            
            # ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ ê²½ìš°ì—ë§Œ ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
            if not is_greeting and not is_simple:
                # ê²€ìƒ‰ í‚¤ì›Œë“œ ì „ì²˜ë¦¬ (1ê¸€ì ì´í•˜ í‚¤ì›Œë“œ ì œì™¸ë¡œ ë…¸ì´ì¦ˆ ê°ì†Œ)
                search_keywords = [word.strip() for word in user_message.lower().split() if len(word.strip()) > 1]
                
                # ê° ë¬¸ì„œì— ëŒ€í•´ ê´€ë ¨ì„± ê²€ì‚¬ (í•„í„°ë§ëœ ë¬¸ì„œë§Œ ëŒ€ìƒ)
                for doc in search_documents:
                    # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš©ì„ í•©ì³ì„œ ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ ìƒì„±
                    doc_text = (doc['title'] + ' ' + doc['content']).lower()
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    match_count = sum(1 for keyword in search_keywords if keyword in doc_text)
                    match_ratio = match_count / len(search_keywords) if search_keywords else 0
                    
                    # ê´€ë ¨ì„± íŒë‹¨ ê¸°ì¤€:
                    # 1) 30% ì´ìƒì˜ í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ê±°ë‚˜
                    # 2) 3ê¸€ì ì´ìƒì˜ ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
                    if match_ratio >= 0.3 or any(keyword in doc_text for keyword in search_keywords if len(keyword) >= 3):
                        relevant_docs.append(doc)
                        # ê´€ë ¨ ë¬¸ì„œì˜ ì´ë¯¸ì§€ë„ í•¨ê»˜ í¬í•¨
                        if doc.get('images'):
                            relevant_images.extend(doc['images'])
                        
                        print(f"ê´€ë ¨ ë¬¸ì„œ ë°œê²¬: {doc['title']} (ë§¤ì¹­ë¥ : {match_ratio:.2f})")
            else:
                print(f"ì¸ì‚¬ë§ ë˜ëŠ” ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤: {user_message}")
            
            # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶œë ¥
            if not is_greeting and not is_simple:
                print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
            print(f"ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")
            
            # Mock AI ì‘ë‹µ ìƒì„±
            if relevant_docs:
                # ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš°: ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ
                doc_titles = [doc['title'] for doc in relevant_docs[:3]]
                if category_id and category_id in CATEGORY_RESPONSES:
                    category_response = random.choice(CATEGORY_RESPONSES[category_id])
                    response = f"{category_response}\n\nì°¸ê³  ë¬¸ì„œ: {', '.join(doc_titles)}\n\nì‚¬ìš©ì ì§ˆë¬¸ '{user_message}'ì— ëŒ€í•œ ìƒì„¸í•œ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                else:
                    response = f"ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œ '{', '.join(doc_titles)}'ë¥¼ ë¶„ì„í•œ ê²°ê³¼, '{user_message}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {random.choice(MOCK_RESPONSES)}"
            else:
                # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°: ì¼ë°˜ì ì¸ ì‘ë‹µ
                print("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                if is_greeting:
                    response = "ì•ˆë…•í•˜ì„¸ìš”! BEACON AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                elif 'ë¬¸ì„œ' in user_message:
                    response = f"í˜„ì¬ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                else:
                    response = random.choice(MOCK_RESPONSES)
        
    except Exception as e:
        logger.error(f"Mock AI ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        search_documents = documents
        if category_id:
            search_documents = [doc for doc in documents if doc.get('category_id') == category_id]
        
        if len(search_documents) == 0:
            if category_id:
                category_name = next((cat['name'] for cat in categories if cat['id'] == category_id), 'ì„ íƒëœ ì¹´í…Œê³ ë¦¬')
                response = f"í˜„ì¬ '{category_name}' ì¹´í…Œê³ ë¦¬ì— ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•œ í›„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            else:
                response = "í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•œ í›„ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        elif 'ì•ˆë…•' in user_message or 'hello' in user_message.lower():
            response = "ì•ˆë…•í•˜ì„¸ìš”! BEACON AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        elif 'ë¬¸ì„œ' in user_message:
            response = f"í˜„ì¬ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
        else:
            response = random.choice(MOCK_RESPONSES)
    
    # ì±„íŒ… ê¸°ë¡ ì €ì¥
    chat_entry = {
        'timestamp': datetime.now().isoformat(),
        'user_message': user_message,
        'ai_response': response
    }
    chat_history.append(chat_entry)
    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (ê¸°ë³¸ ì±„íŒ…)")
    
    return jsonify({
        'response': response,
        'timestamp': chat_entry['timestamp'],
        'images': relevant_images[:5] if relevant_images else [],  # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€
        'referenced_docs': [{'id': doc['id'], 'title': doc.get('original_filename', doc['title']), 'has_file': bool(doc.get('file_path'))} for doc in relevant_docs[:3]] if relevant_docs else []  # ì°¸ì¡°ëœ ë¬¸ì„œ ì •ë³´
    })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """Enhanced file upload with multiple format support and Chroma DB integration"""
    global document_counter
    
    if 'file' not in request.files:
        return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
    
    # Get parameters
    category_id = int(request.form.get('category_id', 4))
    chunk_strategy = request.form.get('chunk_strategy', 'sentence')
    chunk_size = int(request.form.get('chunk_size', 1000))
    chunk_overlap = int(request.form.get('chunk_overlap', 100))
    
    if file and allowed_file(file.filename):
        processing_start_time = time.time()
        
        try:
            # Save file
            filename = secure_filename(file.filename)
            document_counter += 1
            
            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"doc_{document_counter}_{filename}")
            file.save(file_path)
            
            # Extract text using enhanced document processor
            text_content = ""
            extraction_metadata = {}
            images = []
            
            if document_processor:
                try:
                    text_content, extraction_metadata = document_processor.extract_text(file_path)
                    logger.info(f"Text extracted using enhanced processor: {len(text_content)} characters")
                except Exception as e:
                    logger.warning(f"Enhanced extraction failed, falling back to PDF-only: {e}")
                    # Fallback to original PDF extraction for PDFs
                    if file.filename.lower().endswith('.pdf'):
                        with open(file_path, 'rb') as f:
                            text_content = extract_text_from_pdf(f)
                        if text_content:
                            with open(file_path, 'rb') as f:
                                images = extract_images_from_pdf(f, document_counter)
            else:
                # Fallback to original method if processor not available
                if file.filename.lower().endswith('.pdf'):
                    with open(file_path, 'rb') as f:
                        text_content = extract_text_from_pdf(f)
                    if text_content:
                        with open(file_path, 'rb') as f:
                            images = extract_images_from_pdf(f, document_counter)
            
            if not text_content or not text_content.strip():
                return jsonify({'error': 'íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 400
            
            # Create document entry
            new_doc = {
                'id': document_counter,
                'title': filename,
                'content': text_content,
                'type': 'uploaded',
                'images': images,
                'file_path': file_path,
                'original_filename': file.filename,
                'category_id': category_id,
                'extraction_metadata': extraction_metadata,
                'uploaded_at': datetime.now().isoformat(),
                'file_size': os.path.getsize(file_path)
            }
            documents.append(new_doc)
            
            # Enhanced document processing with chunking and embeddings
            chroma_processing_result = None
            legacy_rag_result = None
            
            if CHROMA_ENABLED and chroma_service:
                try:
                    logger.info(f"Processing document with Chroma DB: {filename}")
                    
                    # Generate chunks using configurable strategy (with fallback)
                    if DocumentChunker:
                        if chunk_strategy == 'sentence':
                            chunks = DocumentChunker.chunk_by_sentences(
                                text_content, max_chunk_size=chunk_size, overlap=chunk_overlap
                            )
                        elif chunk_strategy == 'paragraph':
                            chunks = DocumentChunker.chunk_by_paragraphs(
                                text_content, max_chunk_size=chunk_size
                            )
                        elif chunk_strategy == 'token':
                            chunks = DocumentChunker.chunk_by_tokens(
                                text_content, max_tokens=chunk_size//4, overlap_tokens=chunk_overlap//4
                            )
                        else:
                            chunks = DocumentChunker.chunk_by_sentences(text_content, chunk_size, chunk_overlap)
                    else:
                        # Simple fallback chunking
                        words = text_content.split()
                        chunks = []
                        current_chunk = ""
                        for word in words:
                            if len(current_chunk) + len(word) < chunk_size:
                                current_chunk += word + " "
                            else:
                                if current_chunk:
                                    chunks.append(current_chunk.strip())
                                current_chunk = word + " "
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                    
                    # Generate embeddings
                    embeddings = generate_embeddings(chunks)
                    
                    # Store in Chroma DB
                    document_id = f"doc_{document_counter}"
                    success = chroma_service.add_document_chunks(
                        chunks=chunks,
                        embeddings=embeddings,
                        document_id=document_id,
                        document_name=filename,
                        metadata={
                            'category_id': category_id,
                            'chunk_strategy': chunk_strategy,
                            'chunk_size': chunk_size,
                            'file_extension': extraction_metadata.get('file_extension', ''),
                            'original_filename': file.filename
                        }
                    )
                    
                    chroma_processing_result = {
                        'success': success,
                        'chunks_created': len(chunks),
                        'embeddings_generated': len(embeddings),
                        'chunk_strategy': chunk_strategy,
                        'average_chunk_size': sum(len(chunk) for chunk in chunks) // len(chunks) if chunks else 0
                    }
                    
                    new_doc['chunk_count'] = len(chunks)
                    
                    logger.info(f"Chroma DB processing completed: {len(chunks)} chunks created")
                    
                except Exception as e:
                    logger.error(f"Failed to process document with Chroma DB: {e}")
                    chroma_processing_result = {'success': False, 'error': str(e)}
            
            # Legacy RAG processing (keep for compatibility)
            if RAG_ENABLED and rag_engine:
                try:
                    category_settings = next(
                        (cat.get('settings', {}) for cat in categories if cat['id'] == category_id),
                        {'chunk_strategy': chunk_strategy, 'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}
                    )
                    
                    legacy_rag_result = rag_engine.process_document(
                        document_id=str(document_counter),
                        title=filename,
                        content=text_content,
                        category_id=category_id,
                        category_settings=category_settings
                    )
                    
                    logger.info(f"Legacy RAG processing completed")
                    
                except Exception as e:
                    logger.error(f"Failed to process document with legacy RAG: {e}")
            
            # Prepare response
            processing_time = time.time() - processing_start_time
            
            response_data = {
                'success': True,
                'message': f'"{file.filename}" íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'document': {
                    'id': new_doc['id'],
                    'title': new_doc['title'],
                    'preview': text_content[:200] + '...' if len(text_content) > 200 else text_content,
                    'file_size': new_doc['file_size'],
                    'file_extension': extraction_metadata.get('file_extension', ''),
                    'extraction_method': extraction_metadata.get('extraction_method', 'unknown')
                },
                'processing_time': round(processing_time, 2),
                'chroma_enabled': CHROMA_ENABLED,
                'rag_enabled': RAG_ENABLED
            }
            
            # Add processing results
            if chroma_processing_result:
                response_data['chroma_processing'] = chroma_processing_result
            
            if legacy_rag_result:
                response_data['legacy_rag_processing'] = {
                    'chunks_created': legacy_rag_result.chunks_created,
                    'embeddings_generated': legacy_rag_result.embeddings_generated,
                    'processing_time': round(legacy_rag_result.processing_time, 2),
                    'total_tokens': legacy_rag_result.total_tokens
                }
            
            # Add extraction metadata
            if extraction_metadata:
                response_data['extraction_metadata'] = extraction_metadata
            
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"Upload processing failed: {e}")
            return jsonify({'error': f'íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500
    
    return jsonify({'error': f'ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {", ".join(ALLOWED_EXTENSIONS)}'}), 400

@app.route('/api/chat/history')
def get_chat_history():
    return jsonify(chat_history)

@app.route('/api/download/<doc_id>')
def download_file(doc_id):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
    
    Args:
        doc_id (str): ë‹¤ìš´ë¡œë“œí•  ë¬¸ì„œì˜ ID (ë¬¸ìì—´)
        
    Returns:
        íŒŒì¼ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
    """
    logger.info(f"Download request for doc_id: {doc_id}")
    
    # ChromaDBì—ì„œ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    try:
        doc_info = chroma_service.get_document_info(doc_id)
        if not doc_info.get('exists'):
            logger.warning(f"Document not found in ChromaDB: {doc_id}")
            return jsonify({'error': 'ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
        
        # ì›ë³¸ íŒŒì¼ ê²½ë¡œ êµ¬ì„± (ì—…ë¡œë“œ ì‹œ ì €ì¥ëœ ê²½ë¡œ)
        uploads_dir = os.path.join(os.path.dirname(__file__), 'uploads')
        file_path = os.path.join(uploads_dir, f"{doc_id}.txt")
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(file_path):
            logger.info(f"Serving file: {file_path}")
            return send_file(
                file_path,
                as_attachment=True,
                download_name=f"{doc_id}.txt"
            )
        else:
            logger.error(f"File not found on disk: {file_path}")
            return jsonify({'error': 'íŒŒì¼ì´ ë””ìŠ¤í¬ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}), 404
            
    except Exception as e:
        logger.error(f"Error in download_file: {e}")
        return jsonify({'error': f'ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {str(e)}'}), 500
    
    # ê¸°ì¡´ documents ë°°ì—´ì—ì„œë„ ê²€ìƒ‰ (fallback)
    doc = next((d for d in documents if str(d.get('id')) == str(doc_id)), None)
    
    # ë¬¸ì„œê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ íŒŒì¼ ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš°
    if not doc or not doc.get('file_path'):
        return jsonify({'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
    
    # ì‹¤ì œ íŒŒì¼ì´ ë””ìŠ¤í¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(doc['file_path']):
        return jsonify({'error': 'íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}), 404
    
    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ ìƒì„± (ì›ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ)
    return send_file(
        doc['file_path'], 
        as_attachment=True, 
        download_name=doc.get('original_filename', doc['title'])
    )

@app.route('/api/documents/<int:doc_id>', methods=['DELETE'])
def delete_document(doc_id):
    """
    ë¬¸ì„œ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸
    
    Args:
        doc_id (int): ì‚­ì œí•  ë¬¸ì„œì˜ ID
        
    Returns:
        ì„±ê³µ/ì‹¤íŒ¨ ë©”ì‹œì§€
    """
    global documents, document_counter
    
    # ì‚­ì œí•  ë¬¸ì„œ ì°¾ê¸°
    doc_to_delete = next((d for d in documents if d['id'] == doc_id), None)
    
    if not doc_to_delete:
        return jsonify({'error': 'ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
    
    try:
        # íŒŒì¼ ì‚­ì œ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        logger.info(f"Attempting to delete document {doc_id}: {doc_to_delete.get('title', 'Unknown')}")
        logger.info(f"File path in document: {doc_to_delete.get('file_path', 'No file path')}")
        
        if doc_to_delete.get('file_path'):
            file_path = doc_to_delete['file_path']
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"âœ… íŒŒì¼ ì‚­ì œ ì„±ê³µ: {file_path}")
            else:
                logger.warning(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
        
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì‚­ì œ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        image_dir = os.path.join('static', 'images', f'doc_{doc_id}')
        if os.path.exists(image_dir):
            import shutil
            shutil.rmtree(image_dir)
            print(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì‚­ì œë¨: {image_dir}")
        
        # Remove from RAG system if enabled
        rag_deleted_count = 0
        if RAG_ENABLED:
            try:
                rag_deleted_count = rag_engine.delete_document(str(doc_id))
                logger.info(f"Deleted {rag_deleted_count} chunks from legacy RAG system for document {doc_id}")
            except Exception as e:
                logger.error(f"Failed to delete document from legacy RAG system: {e}")
        
        # Remove from ChromaDB if enabled
        chroma_deleted = False
        if CHROMA_ENABLED and chroma_service:
            try:
                # Try different ID formats that might have been used
                possible_ids = [
                    str(doc_id),  # Simple ID
                    f"doc_{doc_id}",  # General upload format
                ]
                
                # If document has index_id, also try knowledge base format
                if doc_to_delete.get('index_id'):
                    possible_ids.append(f"kb_{doc_to_delete['index_id']}_doc_{doc_id}")
                
                for chroma_id in possible_ids:
                    try:
                        deleted = chroma_service.delete_document(chroma_id)
                        if deleted:
                            logger.info(f"âœ… Deleted document from ChromaDB with ID: {chroma_id}")
                            chroma_deleted = True
                            break
                    except Exception as e:
                        logger.debug(f"Failed to delete with ID {chroma_id}: {e}")
                
                if not chroma_deleted:
                    logger.warning(f"âš ï¸ Document {doc_id} not found in ChromaDB with any tried ID format")
            except Exception as e:
                logger.error(f"Failed to delete document from ChromaDB: {e}")
        
        # ë¬¸ì„œ ëª©ë¡ì—ì„œ ì œê±° (global ë³€ìˆ˜ë¥¼ ì§ì ‘ ìˆ˜ì •)
        documents[:] = [d for d in documents if d['id'] != doc_id]
        
        return jsonify({
            'success': True,
            'message': f'ë¬¸ì„œ "{doc_to_delete["title"]}"ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'rag_chunks_deleted': rag_deleted_count if RAG_ENABLED else None
        })
        
    except Exception as e:
        print(f"ë¬¸ì„œ ì‚­ì œ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

# Store knowledge bases in memory (in production, use a database)
# Initialize with default knowledge bases
knowledge_bases_storage = [
]

# Knowledge Base Management APIs
@app.route('/api/knowledge')
def get_knowledge_bases():
    """Get list of knowledge bases"""
    try:
        # Add document count for each knowledge base
        knowledge_bases_with_counts = []
        for kb in knowledge_bases_storage:
            kb_copy = kb.copy()
            kb_copy['document_count'] = len([d for d in documents if d.get('index_id') == kb['id']])
            knowledge_bases_with_counts.append(kb_copy)
        
        return jsonify({
            'success': True,
            'knowledge_bases': knowledge_bases_with_counts
        })
        
    except Exception as e:
        logger.error(f"Failed to get knowledge bases: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/knowledge', methods=['POST'])
def create_knowledge_base():
    """Create a new knowledge base"""
    try:
        data = request.json
        name = data.get('name')
        kb_id = data.get('id')
        description = data.get('description', '')
        
        if not name or not kb_id:
            return jsonify({'error': 'Name and ID are required'}), 400
        
        # Check if ID already exists
        if any(kb['id'] == kb_id for kb in knowledge_bases_storage):
            return jsonify({'error': 'Knowledge base ID already exists'}), 400
        
        new_kb = {
            'id': kb_id,
            'name': name,
            'description': description,
            'status': 'active',
            'created_at': datetime.now().isoformat()
        }
        
        knowledge_bases_storage.append(new_kb)
        
        return jsonify({
            'success': True,
            'knowledge_base': new_kb
        })
        
    except Exception as e:
        logger.error(f"Failed to create knowledge base: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/knowledge/<index_id>', methods=['PUT'])
def update_knowledge_base(index_id):
    """Update an existing knowledge base"""
    try:
        data = request.json
        
        # Find the knowledge base
        kb_index = next((i for i, kb in enumerate(knowledge_bases_storage) if kb['id'] == index_id), None)
        
        if kb_index is None:
            return jsonify({'error': 'Knowledge base not found'}), 404
        
        # Update fields
        if 'name' in data:
            knowledge_bases_storage[kb_index]['name'] = data['name']
        if 'description' in data:
            knowledge_bases_storage[kb_index]['description'] = data['description']
        if 'status' in data:
            knowledge_bases_storage[kb_index]['status'] = data['status']
        
        knowledge_bases_storage[kb_index]['updated_at'] = datetime.now().isoformat()
        
        return jsonify({
            'success': True,
            'knowledge_base': knowledge_bases_storage[kb_index]
        })
        
    except Exception as e:
        logger.error(f"Failed to update knowledge base: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/knowledge/<index_id>', methods=['DELETE'])
def delete_knowledge_base(index_id):
    """Delete a knowledge base"""
    try:
        # Check if there are documents in this knowledge base
        kb_documents = [d for d in documents if d.get('index_id') == index_id]
        if kb_documents:
            return jsonify({'error': 'Cannot delete knowledge base with documents'}), 400
        
        # Find and remove the knowledge base
        kb_index = next((i for i, kb in enumerate(knowledge_bases_storage) if kb['id'] == index_id), None)
        
        if kb_index is None:
            return jsonify({'error': 'Knowledge base not found'}), 404
        
        deleted_kb = knowledge_bases_storage.pop(kb_index)
        
        return jsonify({
            'success': True,
            'deleted': deleted_kb
        })
        
    except Exception as e:
        logger.error(f"Failed to delete knowledge base: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/knowledge/<index_id>/documents')
def get_knowledge_base_documents(index_id):
    """Get documents for a specific knowledge base"""
    try:
        # Filter documents by index_id
        kb_documents = [d for d in documents if d.get('index_id') == index_id]
        
        # Format documents for frontend
        formatted_docs = []
        for doc in kb_documents:
            formatted_docs.append({
                'id': doc['id'],
                'file_name': doc['title'],
                'file_size': doc.get('file_size', 0),
                'uploaded_at': doc.get('uploaded_at', datetime.now().isoformat()),
                'status': doc.get('status', 'Success'),
                'chunk_count': doc.get('chunk_count', 1),
                'index_id': index_id
            })
        
        return jsonify({
            'success': True,
            'documents': formatted_docs,
            'total': len(formatted_docs)
        })
        
    except Exception as e:
        logger.error(f"Failed to get documents for knowledge base {index_id}: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/knowledge/upload', methods=['POST'])
def upload_to_knowledge_base():
    """Upload file to specific knowledge base"""
    if 'file' not in request.files:
        return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
    
    file = request.files['file']
    index_id = request.form.get('index_id')
    
    if file.filename == '':
        return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
    if not index_id:
        return jsonify({'error': 'Knowledge base IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
    
    try:
        # Secure filename
        filename = secure_filename(file.filename)
        
        # Create unique filename
        timestamp = str(int(time.time()))
        unique_filename = f"{timestamp}_{filename}"
        
        # Ensure upload directory exists
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        
        # Save file
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
        file.save(file_path)
        
        # Get file size
        file_size = os.path.getsize(file_path)
        
        # Process file content
        content = ""
        try:
            if filename.lower().endswith('.pdf'):
                # PDF processing
                with open(file_path, 'rb') as pdf_file:
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    for page in pdf_reader.pages:
                        content += page.extract_text() + "\n"
            elif filename.lower().endswith(('.txt', '.md')):
                # Text file processing
                with open(file_path, 'r', encoding='utf-8') as txt_file:
                    content = txt_file.read()
            else:
                content = f"File: {filename} (Content not extracted)"
                
        except Exception as e:
            logger.warning(f"Could not extract content from {filename}: {e}")
            content = f"File: {filename} (Content extraction failed)"
        
        # Create document record
        new_doc = {
            'id': len(documents) + 1,
            'title': filename,
            'content': content[:1000] + ('...' if len(content) > 1000 else ''),
            'file_path': file_path,
            'file_size': file_size,
            'uploaded_at': datetime.now().isoformat(),
            'category_id': None,
            'index_id': index_id,
            'status': 'Success',
            'chunk_count': max(1, len(content) // 1000)  # Estimate chunks
        }
        
        documents.append(new_doc)
        
        # Add to ChromaDB if enabled, otherwise fallback to legacy RAG
        chunks_added = 0
        if CHROMA_ENABLED and chroma_service and content.strip():
            try:
                logger.info("=== RAG ì¤€ë¹„ë‹¨ê³„ ì‹œì‘ ===")
                logger.info("1. âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
                logger.info("2. âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ - %d ë¬¸ì ì¶”ì¶œ", len(content))
                
                # Get processing parameters
                chunk_strategy = request.form.get('chunking_strategy', 'sentence')
                chunk_size = int(request.form.get('chunk_size', 1000))
                chunk_overlap = int(request.form.get('chunk_overlap', 100))
                
                logger.info("3. ğŸ”„ ì²­í¬ ë¶„í•  ì‹œì‘ - ì „ëµ: %s, í¬ê¸°: %d", chunk_strategy, chunk_size)
                
                # Generate chunks using DocumentChunker
                if DocumentChunker:
                    if chunk_strategy == 'sentence':
                        chunks = DocumentChunker.chunk_by_sentences(
                            content, max_chunk_size=chunk_size, overlap=chunk_overlap
                        )
                    elif chunk_strategy == 'paragraph':
                        chunks = DocumentChunker.chunk_by_paragraphs(
                            content, max_chunk_size=chunk_size
                        )
                    elif chunk_strategy == 'token':
                        chunks = DocumentChunker.chunk_by_tokens(
                            content, max_tokens=chunk_size//4, overlap_tokens=chunk_overlap//4
                        )
                    else:
                        chunks = DocumentChunker.chunk_by_sentences(content, chunk_size, chunk_overlap)
                else:
                    # Simple fallback chunking
                    words = content.split()
                    chunks = []
                    current_chunk = ""
                    for word in words:
                        if len(current_chunk) + len(word) < chunk_size:
                            current_chunk += word + " "
                        else:
                            if current_chunk:
                                chunks.append(current_chunk.strip())
                            current_chunk = word + " "
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                
                logger.info("3. âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ - %dê°œ ì²­í¬ ìƒì„±", len(chunks))
                
                # Generate embeddings
                logger.info("4. ğŸ”„ Bedrock ì„ë² ë”© ìƒì„± ì‹œì‘ - %dê°œ ì²­í¬", len(chunks))
                embeddings = generate_embeddings(chunks)
                logger.info("4. âœ… Bedrock ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                
                # Store in Chroma DB
                logger.info("5. ğŸ”„ ChromaDB ì €ì¥ ì‹œì‘")
                document_id = f"kb_{index_id}_doc_{new_doc['id']}"
                success = chroma_service.add_document_chunks(
                    chunks=chunks,
                    embeddings=embeddings,
                    document_id=document_id,
                    document_name=filename,
                    metadata={
                        'index_id': index_id,
                        'chunk_strategy': chunk_strategy,
                        'chunk_size': chunk_size,
                        'original_filename': filename,
                        'file_path': file_path,
                        'uploaded_at': new_doc['uploaded_at']
                    }
                )
                
                if success:
                    chunks_added = len(chunks)
                    new_doc['chunk_count'] = len(chunks)
                    logger.info("5. âœ… ChromaDB ì €ì¥ ì™„ë£Œ - %dê°œ ì²­í¬ ì €ì¥", chunks_added)
                    logger.info("=== RAG ì¤€ë¹„ë‹¨ê³„ ì™„ë£Œ ===")
                else:
                    logger.error("5. âŒ ChromaDB ì €ì¥ ì‹¤íŒ¨")
                    logger.error("=== RAG ì¤€ë¹„ë‹¨ê³„ ì‹¤íŒ¨ ===")
                    
            except Exception as e:
                logger.error(f"Failed to add document to ChromaDB: {e}")
                # Fallback to legacy RAG system
                try:
                    chunks_added = rag_engine.add_document(
                        doc_id=str(new_doc['id']),
                        content=content,
                        metadata={
                            'title': filename,
                            'file_path': file_path,
                            'index_id': index_id,
                            'category_id': new_doc.get('category_id'),
                            'uploaded_at': new_doc['uploaded_at']
                        }
                    )
                    logger.info(f"Fallback: Added {chunks_added} chunks to legacy RAG system")
                except Exception as e2:
                    logger.error(f"Both ChromaDB and legacy RAG failed: {e2}")
                    
        elif RAG_ENABLED and content.strip():
            # Use legacy RAG system only
            try:
                chunks_added = rag_engine.add_document(
                    doc_id=str(new_doc['id']),
                    content=content,
                    metadata={
                        'title': filename,
                        'file_path': file_path,
                        'index_id': index_id,
                        'category_id': new_doc.get('category_id'),
                        'uploaded_at': new_doc['uploaded_at']
                    }
                )
                logger.info(f"Added {chunks_added} chunks to legacy RAG system for document {new_doc['id']}")
            except Exception as e:
                logger.error(f"Failed to add document to legacy RAG system: {e}")
        
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {filename} (ID: {new_doc['id']}, Index: {index_id})")
        
        return jsonify({
            'success': True,
            'message': f'íŒŒì¼ "{filename}"ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'document_id': new_doc['id'],
            'chunks_added': chunks_added,
            'document': {
                'id': new_doc['id'],
                'file_name': new_doc['title'],
                'file_size': new_doc['file_size'],
                'uploaded_at': new_doc['uploaded_at'],
                'status': new_doc['status'],
                'chunk_count': new_doc['chunk_count'],
                'index_id': index_id
            }
        })
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/api/documents/bulk', methods=['DELETE'])
def delete_multiple_documents():
    """Delete multiple documents"""
    global documents
    
    data = request.get_json()
    document_ids = data.get('document_ids', [])
    
    if not document_ids:
        return jsonify({'error': 'Document IDs are required'}), 400
    
    try:
        deleted_count = 0
        deleted_docs = []
        
        for doc_id in document_ids:
            # Find document
            doc_to_delete = next((d for d in documents if d['id'] == doc_id), None)
            if not doc_to_delete:
                continue
                
            deleted_docs.append(doc_to_delete['title'])
            
            # Delete file if exists
            try:
                if 'file_path' in doc_to_delete and os.path.exists(doc_to_delete['file_path']):
                    os.remove(doc_to_delete['file_path'])
            except Exception as e:
                logger.warning(f"Failed to delete file for document {doc_id}: {e}")
            
            # Delete from legacy RAG system
            if RAG_ENABLED:
                try:
                    rag_engine.delete_document(str(doc_id))
                    logger.info(f"Deleted document {doc_id} from legacy RAG system")
                except Exception as e:
                    logger.error(f"Failed to delete document {doc_id} from legacy RAG system: {e}")
            
            # Delete from ChromaDB
            if CHROMA_ENABLED and chroma_service:
                try:
                    # Try different ID formats
                    possible_ids = [
                        str(doc_id),
                        f"doc_{doc_id}"
                    ]
                    if doc_to_delete.get('index_id'):
                        possible_ids.append(f"kb_{doc_to_delete['index_id']}_doc_{doc_id}")
                    
                    for chroma_id in possible_ids:
                        try:
                            if chroma_service.delete_document(chroma_id):
                                logger.info(f"Deleted document {doc_id} from ChromaDB with ID: {chroma_id}")
                                break
                        except:
                            pass
                except Exception as e:
                    logger.error(f"Failed to delete document {doc_id} from ChromaDB: {e}")
            
            deleted_count += 1
        
        # Remove documents from list (global ë³€ìˆ˜ë¥¼ ì§ì ‘ ìˆ˜ì •)
        documents[:] = [d for d in documents if d['id'] not in document_ids]
        
        return jsonify({
            'success': True,
            'message': f'{deleted_count} documents deleted successfully',
            'deleted_documents': deleted_docs,
            'deleted_count': deleted_count
        })
        
    except Exception as e:
        logger.error(f"Bulk delete error: {e}")
        return jsonify({'error': f'Failed to delete documents: {str(e)}'}), 500

@app.route('/api/documents/<int:doc_id>/reprocess', methods=['POST'])
def reprocess_document(doc_id):
    """Reprocess a document with new settings"""
    try:
        # Find document
        doc = next((d for d in documents if d['id'] == doc_id), None)
        if not doc:
            return jsonify({'error': 'Document not found'}), 404
        
        # Get processing options
        data = request.get_json() or {}
        
        # Update document status
        doc['status'] = 'Processing'
        
        # In a real implementation, you would:
        # 1. Re-extract content with new settings
        # 2. Re-chunk the document
        # 3. Update embeddings
        # 4. Update vector store
        
        # For now, simulate processing
        import time
        time.sleep(1)  # Simulate processing time
        
        # Update status
        doc['status'] = 'Success'
        doc['processed_at'] = datetime.now().isoformat()
        
        logger.info(f"Document {doc_id} reprocessed successfully")
        
        return jsonify({
            'success': True,
            'message': f'Document "{doc["title"]}" queued for reprocessing',
            'document_id': doc_id
        })
        
    except Exception as e:
        logger.error(f"Document reprocessing error: {e}")
        return jsonify({'error': f'Failed to reprocess document: {str(e)}'}), 500

# Enhanced Chroma DB API endpoints
@app.route('/api/chroma/search', methods=['POST'])
def chroma_search():
    """Search for similar documents using Chroma DB"""
    if not CHROMA_ENABLED or not chroma_service:
        return jsonify({'error': 'Chroma DB not available'}), 503
    
    try:
        data = request.get_json()
        query_text = data.get('query', '')
        n_results = min(data.get('n_results', 5), 20)  # Limit to 20 results
        document_filter = data.get('document_filter')
        
        if not query_text:
            return jsonify({'error': 'Query text is required'}), 400
        
        # Generate embedding for query
        query_embeddings = generate_embeddings([query_text])
        if not query_embeddings:
            return jsonify({'error': 'Failed to generate query embedding'}), 500
        
        # Search similar chunks
        search_results = chroma_service.search_similar_chunks(
            query_embedding=query_embeddings[0],
            n_results=n_results,
            document_filter=document_filter
        )
        
        return jsonify({
            'success': True,
            'query': query_text,
            'results': search_results
        })
        
    except Exception as e:
        logger.error(f"Chroma search failed: {e}")
        return jsonify({'error': f'Search failed: {str(e)}'}), 500

@app.route('/api/chroma/stats')
def chroma_stats():
    """Get Chroma DB collection statistics"""
    if not CHROMA_ENABLED or not chroma_service:
        return jsonify({'error': 'Chroma DB not available'}), 503
    
    try:
        stats = chroma_service.get_collection_stats()
        documents_list = chroma_service.list_all_documents()
        
        return jsonify({
            'success': True,
            'stats': stats,
            'documents': documents_list
        })
        
    except Exception as e:
        logger.error(f"Failed to get Chroma stats: {e}")
        return jsonify({'error': f'Failed to get statistics: {str(e)}'}), 500

@app.route('/api/chroma/document/<document_id>')
def get_chroma_document_info(document_id):
    """Get information about a specific document in Chroma DB"""
    if not CHROMA_ENABLED or not chroma_service:
        return jsonify({'error': 'Chroma DB not available'}), 503
    
    try:
        doc_info = chroma_service.get_document_info(document_id)
        return jsonify({
            'success': True,
            'document_info': doc_info
        })
        
    except Exception as e:
        logger.error(f"Failed to get document info: {e}")
        return jsonify({'error': f'Failed to get document info: {str(e)}'}), 500

@app.route('/api/chroma/document/<document_id>', methods=['DELETE'])
def delete_chroma_document(document_id):
    """Delete a document from Chroma DB"""
    if not CHROMA_ENABLED or not chroma_service:
        return jsonify({'error': 'Chroma DB not available'}), 503
    
    try:
        success = chroma_service.delete_document(document_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': f'Document {document_id} deleted from Chroma DB'
            })
        else:
            return jsonify({
                'success': False,
                'message': f'Document {document_id} not found in Chroma DB'
            }), 404
        
    except Exception as e:
        logger.error(f"Failed to delete document: {e}")
        return jsonify({'error': f'Failed to delete document: {str(e)}'}), 500

@app.route('/api/document/formats')
def get_supported_formats():
    """Get list of supported document formats"""
    formats = list(ALLOWED_EXTENSIONS)
    
    format_info = {}
    if document_processor:
        for fmt in formats:
            format_info[fmt] = {
                'supported': True,
                'description': f'{fmt.upper()} document format'
            }
    
    return jsonify({
        'success': True,
        'supported_formats': formats,
        'format_info': format_info,
        'chroma_enabled': CHROMA_ENABLED,
        'rag_enabled': RAG_ENABLED
    })

@app.route('/api/weather')
def get_weather():
    # ìƒ˜í”Œ ë‚ ì”¨ ë°ì´í„°
    return jsonify({
        'temperature': '3Â°C',
        'location': 'ì•ˆì–‘ì‹œ ë™êµ¬',
        'condition': 'íë¦¼',
        'range': '5Â°C/-1Â°C'
    })

@app.route('/api/chroma/clear', methods=['POST'])
def clear_chroma():
    """Clear all documents from ChromaDB collection"""
    try:
        if chroma_service.clear_collection():
            logger.info("ChromaDB collection cleared successfully")
            return jsonify({
                'success': True,
                'message': 'ChromaDB collection cleared successfully'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'Failed to clear ChromaDB collection'
            }), 500
    except Exception as e:
        logger.error(f"Error clearing ChromaDB: {e}")
        return jsonify({
            'success': False,
            'message': f'Error clearing ChromaDB: {str(e)}'
        }), 500

@app.route('/api/chroma/reset', methods=['POST'])
def reset_chroma():
    """Reset ChromaDB collection by deleting and recreating it"""
    try:
        if chroma_service.reset_collection():
            logger.info("ChromaDB collection reset successfully")
            return jsonify({
                'success': True,
                'message': 'ChromaDB collection reset successfully'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'Failed to reset ChromaDB collection'
            }), 500
    except Exception as e:
        logger.error(f"Error resetting ChromaDB: {e}")
        return jsonify({
            'success': False,
            'message': f'Error resetting ChromaDB: {str(e)}'
        }), 500

@app.route('/api/chroma/stats', methods=['GET'])
def get_chroma_stats():
    """Get ChromaDB collection statistics"""
    try:
        stats = chroma_service.get_collection_stats()
        return jsonify({
            'success': True,
            'stats': stats
        })
    except Exception as e:
        logger.error(f"Error getting ChromaDB stats: {e}")
        return jsonify({
            'success': False,
            'message': f'Error getting ChromaDB stats: {str(e)}'
        }), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)