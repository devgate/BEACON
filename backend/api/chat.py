"""
Chat API module for BEACON
Handles all chat-related endpoints and AI interactions
"""
from flask import Blueprint, request, jsonify
from datetime import datetime
import random
import logging
import time
from typing import Optional

logger = logging.getLogger(__name__)

# Create Blueprint
chat_bp = Blueprint('chat', __name__)

# Mock AI responses (for fallback)
MOCK_RESPONSES = [
    "ì•ˆë…•í•˜ì„¸ìš”! BEACON AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    "ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•œ ê²°ê³¼, ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
    "ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê²€í† í•´ë³´ë‹ˆ í¥ë¯¸ë¡œìš´ ë‚´ìš©ì´ ë§ë„¤ìš”. íŠ¹ì • ì„¹ì…˜ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "ë¬¸ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½í•˜ìë©´, í•µì‹¬ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
    "í•´ë‹¹ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ë„¤, ì°¾ì•˜ìŠµë‹ˆë‹¤! ìƒì„¸í•œ ì„¤ëª…ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
]

# Storage for chat history
chat_history = []

def init_chat_module(app_context):
    """Initialize chat module with app context"""
    global bedrock_service, vector_store, rag_engine, chroma_service
    global documents, generate_embeddings
    global RAG_ENABLED, CHROMA_ENABLED
    
    bedrock_service = app_context['bedrock_service']
    vector_store = app_context['vector_store']
    rag_engine = app_context['rag_engine']
    chroma_service = app_context.get('chroma_service')
    documents = app_context['documents']
    generate_embeddings = app_context['generate_embeddings']
    RAG_ENABLED = app_context['RAG_ENABLED']
    CHROMA_ENABLED = app_context['CHROMA_ENABLED']

@chat_bp.route('/api/chat', methods=['POST'])
def chat():
    """
    Enhanced chat API endpoint with Bedrock RAG integration
    Supports model selection, cost tracking, and real AI responses
    """
    logger.info("ğŸ“ Chat API í˜¸ì¶œë¨ - ìš”ì²­ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    data = request.get_json()
    user_message = data.get('message', '')
    model_id = data.get('model_id', None)
    settings = data.get('settings', {})
    
    # Extract settings
    temperature = float(settings.get('temperature', 0.7))
    max_tokens = int(settings.get('max_tokens', 2048))
    use_rag = settings.get('use_rag', True)
    knowledge_base_id = settings.get('knowledge_base_id', None)
    top_k_documents = int(settings.get('top_k_documents', 5))
    
    try:
        logger.info("ğŸ” RAG_ENABLED ìƒíƒœ: %s", RAG_ENABLED)
        if RAG_ENABLED:
            # Check if ChromaDB RAG should be used
            logger.info("ğŸ” RAG ì¡°ê±´ ì²´í¬: use_rag=%s, knowledge_base_id=%s, CHROMA_ENABLED=%s, chroma_service=%s", 
                       use_rag, knowledge_base_id, CHROMA_ENABLED, chroma_service is not None)
            use_chroma_rag = use_rag and knowledge_base_id and CHROMA_ENABLED and chroma_service
            logger.info("ğŸ¯ ChromaDB RAG ì‚¬ìš© ì—¬ë¶€: %s", use_chroma_rag)
            
            if use_chroma_rag:
                return _handle_chroma_rag(
                    user_message, model_id, temperature, max_tokens,
                    knowledge_base_id, top_k_documents
                )
            elif use_rag and documents:
                return _handle_legacy_rag(
                    user_message, model_id,
                    top_k_documents, temperature, max_tokens
                )
            else:
                return _handle_general_conversation(
                    user_message, model_id, temperature, max_tokens
                )
        else:
            # RAG_ENABLED is False - fallback to mock responses
            logger.warning("âŒ RAG_ENABLEDê°€ Falseì…ë‹ˆë‹¤ - Mock ì‘ë‹µ ì‹œìŠ¤í…œ ì‚¬ìš©")
            return _generate_mock_response(user_message)
            
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}")
        return _generate_mock_response(user_message, error=str(e))

def _handle_chroma_rag(user_message, model_id, temperature, max_tokens,
                      knowledge_base_id, top_k_documents):
    """Handle ChromaDB RAG system"""
    logger.info("=== RAG ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
    logger.info("1. âœ… ì§ˆë¬¸ ì…ë ¥: %s", user_message[:50] + "..." if len(user_message) > 50 else user_message)
    
    start_time = time.time()
    
    # Generate embedding for the user query
    logger.info("2. ğŸ”„ ì§ˆë¬¸ Bedrock ì„ë² ë”© ìƒì„± ì‹œì‘")
    query_embeddings = generate_embeddings([user_message])
    if not query_embeddings or len(query_embeddings) == 0:
        raise Exception("Failed to generate query embedding")
    
    query_embedding = query_embeddings[0]
    logger.info("2. âœ… ì§ˆë¬¸ Bedrock ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    # Search similar chunks in ChromaDB
    logger.info("3. ğŸ”„ ChromaDB ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘ - ì§€ì‹ë² ì´ìŠ¤: %s", knowledge_base_id)
    search_results = chroma_service.search_similar_chunks(
        query_embedding=query_embedding,
        n_results=top_k_documents,
        document_filter=f"kb_{knowledge_base_id}_doc_"
    )
    
    if search_results['total_results'] > 0:
        logger.info("3. âœ… ChromaDB ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ - %dê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬", search_results['total_results'])
        
        # Build context from retrieved chunks
        context_chunks = search_results['chunks']
        metadatas = search_results['metadatas']
        distances = search_results['distances']
        
        logger.info("4. ğŸ”„ ê²€ìƒ‰ëœ ì²­í¬ë¡œ LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹œì‘")
        context = "\n\n".join([
            f"ë¬¸ì„œ '{meta.get('document_name', 'Unknown')}' - ê´€ë ¨ë„: {1-distance:.3f}\n{chunk}"
            for chunk, meta, distance in zip(context_chunks, metadatas, distances)
        ])
        
        # Set default model if not specified
        if not model_id:
            model_id = _get_default_text_model()
        
        # Create RAG system prompt
        system_prompt = f"""ë‹¹ì‹ ì€ BEACON AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”
4. ê°€ëŠ¥í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”
5. ê´€ë ¨ëœ ë¬¸ì„œ ì„¹ì…˜ì´ë‚˜ ë‚´ìš©ì„ ì¸ìš©í•  ë•ŒëŠ” í•´ë‹¹ ë¬¸ì„œëª…ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”"""
        
        logger.info("4. âœ… LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ")
        logger.info("5. ğŸ”„ Bedrock LLM ì‘ë‹µ ìƒì„± ì‹œì‘ - ëª¨ë¸: %s", model_id)
        
        # Generate response using Bedrock
        response_data = bedrock_service.invoke_model(
            model_id=model_id,
            prompt=user_message,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        logger.info("5. âœ… Bedrock LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        
        processing_time = time.time() - start_time
        
        # Format referenced docs
        referenced_docs = []
        for meta, distance in zip(metadatas, distances):
            referenced_docs.append({
                'id': meta.get('document_id', 'unknown'),
                'title': meta.get('document_name', 'Unknown Document'),
                'has_file': True,
                'relevance_score': 1 - distance,
                'chunk_index': meta.get('chunk_index', 0)
            })
        
        # Save to chat history
        chat_entry = {
            'timestamp': datetime.now().isoformat(),
            'user_message': user_message,
            'ai_response': response_data['text'],
            'model_used': model_id,
            'knowledge_base_id': knowledge_base_id,
            'tokens_used': response_data.get('usage', {}),
            'cost_estimate': response_data.get('cost', {}),
            'confidence_score': sum(1-d for d in distances) / len(distances) if distances else 0.0,
            'processing_time': processing_time,
            'sources_count': len(context_chunks)
        }
        chat_history.append(chat_entry)
        logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ")
        
        return jsonify({
            'response': response_data['text'],
            'model_used': model_id,
            'timestamp': chat_entry['timestamp'],
            'tokens_used': response_data.get('usage', {}),
            'cost_estimate': response_data.get('cost', {}),
            'confidence_score': chat_entry['confidence_score'],
            'processing_time': processing_time,
            'images': [],
            'referenced_docs': referenced_docs[:3],
            'rag_enabled': True
        })
    else:
        # No relevant documents found
        logger.warning(f"No relevant documents found in ChromaDB for query: {user_message[:50]}...")
        return _handle_no_documents_found(
            user_message, model_id, temperature, max_tokens,
            knowledge_base_id
        )

def _handle_legacy_rag(user_message, model_id,
                       top_k_documents, temperature, max_tokens):
    """Handle legacy RAG system"""
    logger.info(f"Processing legacy RAG query: {user_message[:50]}... with model: {model_id}")
    
    response_data = rag_engine.query(
        query_text=user_message,
        model_id=model_id,
        top_k_documents=top_k_documents,
        temperature=temperature,
        max_tokens=max_tokens,
        include_sources=True
    )
    
    # Format sources for frontend
    referenced_docs = []
    images = []
    
    for source in response_data.sources:
        doc_info = next((doc for doc in documents if str(doc['id']) == source.document_id), None)
        
        referenced_docs.append({
            'id': source.document_id,
            'title': source.metadata.get('title', 'Unknown Document'),
            'has_file': bool(doc_info and doc_info.get('file_path')),
            'relevance_score': source.similarity_score,
            'chunk_index': source.chunk_index
        })
        
        if doc_info and doc_info.get('images'):
            images.extend(doc_info['images'][:2])
    
    # Save to chat history
    chat_entry = {
        'timestamp': datetime.now().isoformat(),
        'user_message': user_message,
        'ai_response': response_data.response,
        'model_used': response_data.model_used,
        'tokens_used': response_data.tokens_used,
        'cost_estimate': response_data.cost_estimate,
        'confidence_score': response_data.confidence_score,
        'processing_time': response_data.processing_time,
        'sources_count': len(response_data.sources)
    }
    chat_history.append(chat_entry)
    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (RAG ë¹„í™œì„±í™”)")
    
    return jsonify({
        'response': response_data.response,
        'model_used': response_data.model_used,
        'timestamp': chat_entry['timestamp'],
        'tokens_used': response_data.tokens_used,
        'cost_estimate': response_data.cost_estimate,
        'confidence_score': response_data.confidence_score,
        'processing_time': response_data.processing_time,
        'images': images[:5],
        'referenced_docs': referenced_docs[:3],
        'rag_enabled': True
    })

def _handle_general_conversation(user_message, model_id, temperature, max_tokens):
    """Handle general conversation without RAG"""
    logger.info(f"Processing general conversation with Bedrock: {user_message[:50]}... with model: {model_id}")
    
    start_time = time.time()
    
    # Set default model if not specified
    if not model_id:
        model_id = _get_default_text_model()
    
    # Create a general conversation system prompt
    system_prompt = """ë‹¹ì‹ ì€ BEACON AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”. 
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ë˜, ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ ì–¸ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§ì”€í•´ì£¼ì„¸ìš”."""
    
    # Invoke Bedrock model directly
    response_data = bedrock_service.invoke_model(
        model_id=model_id,
        prompt=user_message,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )
    
    processing_time = time.time() - start_time
    
    # Save to chat history
    chat_entry = {
        'timestamp': datetime.now().isoformat(),
        'user_message': user_message,
        'ai_response': response_data['text'],
        'model_used': model_id,
        'tokens_used': response_data.get('usage', {}),
        'cost_estimate': response_data.get('cost', {}),
        'confidence_score': 1.0,
        'processing_time': processing_time,
        'sources_count': 0
    }
    chat_history.append(chat_entry)
    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (ì´ë¯¸ì§€ í¬í•¨)")
    
    return jsonify({
        'response': response_data['text'],
        'model_used': model_id,
        'timestamp': chat_entry['timestamp'],
        'tokens_used': response_data.get('usage', {}),
        'cost_estimate': response_data.get('cost', {}),
        'confidence_score': 1.0,
        'processing_time': processing_time,
        'images': [],
        'referenced_docs': [],
        'rag_enabled': False
    })

def _handle_no_documents_found(user_message, model_id, temperature, max_tokens,
                              knowledge_base_id):
    """Handle case when no relevant documents are found"""
    logger.info(f"Falling back to general conversation with Bedrock: {user_message[:50]}...")
    
    start_time = time.time()
    
    # Set default model if not specified
    if not model_id:
        model_id = _get_default_text_model()
    
    # Create a system prompt for no documents found
    system_prompt = """ë‹¹ì‹ ì€ BEACON AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”. 
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ë˜, ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ ì–¸ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§ì”€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìê°€ ë¬¸ì„œë‚˜ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í–ˆì§€ë§Œ, ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. 
ì´ ê²½ìš° ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ê³  ì¼ë°˜ì ì¸ ì •ë³´ë‚˜ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
    
    # Invoke Bedrock model
    response_data = bedrock_service.invoke_model(
        model_id=model_id,
        prompt=user_message,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature
    )
    
    processing_time = time.time() - start_time
    
    # Save to chat history
    chat_entry = {
        'timestamp': datetime.now().isoformat(),
        'user_message': user_message,
        'ai_response': response_data['text'],
        'model_used': model_id,
        'knowledge_base_id': knowledge_base_id,
        'tokens_used': response_data.get('usage', {}),
        'cost_estimate': response_data.get('cost', {}),
        'confidence_score': 0.5,
        'processing_time': processing_time,
        'rag_enabled': True,
        'sources_count': 0
    }
    chat_history.append(chat_entry)
    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (ë¬¸ì„œ ì—†ìŒ - ì¼ë°˜ ì‘ë‹µ)")
    
    return jsonify({
        'response': response_data['text'],
        'model_used': model_id,
        'timestamp': chat_entry['timestamp'],
        'tokens_used': response_data.get('usage', {}),
        'cost_estimate': response_data.get('cost', {}),
        'confidence_score': chat_entry['confidence_score'],
        'processing_time': processing_time,
        'images': [],
        'referenced_docs': [],
        'rag_enabled': True,
        'sources_found': False
    })

def _get_default_text_model():
    """Get default text generation model"""
    available_models = bedrock_service.get_available_models()
    text_models = [
        model for model in available_models 
        if 'TEXT' in model.output_modalities and 'EMBEDDING' not in model.output_modalities
    ]
    if text_models:
        claude_models = [m for m in text_models if 'claude' in m.model_id.lower()]
        nova_models = [m for m in text_models if 'nova' in m.model_id.lower()]
        
        if claude_models:
            return claude_models[0].model_id
        elif nova_models:
            return nova_models[0].model_id
        else:
            return text_models[0].model_id
    else:
        raise Exception("No text generation models available")

def _generate_mock_response(user_message: str, error: str = None):
    """Generate mock response for fallback mode"""
    try:
        if error:
            logger.warning(f"Generating mock response due to error: {error}")
        
        relevant_docs = []
        relevant_images = []
        
        search_documents = documents
        
        if len(search_documents) == 0:
            response = "í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•œ í›„ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        else:
            greeting_words = ['ì•ˆë…•', 'hello', 'hi', 'ë°˜ê°€ì›Œ', 'ì˜ì§€ë‚´', 'ì–´ë–»ê²Œ']
            simple_questions = ['ë­ì•¼', 'ë­”ê°€', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ']
            
            user_message_lower = user_message.lower()
            is_greeting = any(word in user_message_lower for word in greeting_words)
            is_simple = len(user_message.split()) <= 3 and any(word in user_message_lower for word in simple_questions)
            
            if not is_greeting and not is_simple:
                search_keywords = [word.strip() for word in user_message.lower().split() if len(word.strip()) > 1]
                
                for doc in search_documents:
                    doc_text = (doc['title'] + ' ' + doc['content']).lower()
                    match_count = sum(1 for keyword in search_keywords if keyword in doc_text)
                    match_ratio = match_count / len(search_keywords) if search_keywords else 0
                    
                    if match_ratio >= 0.3 or any(keyword in doc_text for keyword in search_keywords if len(keyword) >= 3):
                        relevant_docs.append(doc)
                        if doc.get('images'):
                            relevant_images.extend(doc['images'])
                        print(f"ê´€ë ¨ ë¬¸ì„œ ë°œê²¬: {doc['title']} (ë§¤ì¹­ë¥ : {match_ratio:.2f})")
            
            if relevant_docs:
                doc_titles = [doc['title'] for doc in relevant_docs[:3]]
                response = f"ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œ '{', '.join(doc_titles)}'ë¥¼ ë¶„ì„í•œ ê²°ê³¼, '{user_message}'ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {random.choice(MOCK_RESPONSES)}"
            else:
                if is_greeting:
                    response = "ì•ˆë…•í•˜ì„¸ìš”! BEACON AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì—…ë¡œë“œí•˜ì‹  ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                elif 'ë¬¸ì„œ' in user_message:
                    response = f"í˜„ì¬ {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                else:
                    response = random.choice(MOCK_RESPONSES)
        
    except Exception as e:
        logger.error(f"Mock AI ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        response = random.choice(MOCK_RESPONSES)
    
    chat_entry = {
        'timestamp': datetime.now().isoformat(),
        'user_message': user_message,
        'ai_response': response
    }
    chat_history.append(chat_entry)
    logger.info("6. âœ… ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ì†¡ ì™„ë£Œ (ê¸°ë³¸ ì±„íŒ…)")
    
    return jsonify({
        'response': response,
        'timestamp': chat_entry['timestamp'],
        'images': relevant_images[:5] if relevant_images else [],
        'referenced_docs': [{'id': doc['id'], 'title': doc.get('original_filename', doc['title']), 'has_file': bool(doc.get('file_path'))} for doc in relevant_docs[:3]] if relevant_docs else []
    })

@chat_bp.route('/api/chat/history')
def get_chat_history():
    """Get chat history"""
    return jsonify(chat_history)